<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

  <!-- If you plan on running YARN, you must set this property to the value of yarn -->
  <property><name>mapreduce.framework.name</name><value>yarn</value></property>

  <!--  the most important properties for jobhistory -->
  <property><name>mapreduce.jobhistory.address</name><value>{{  groups['cdh5-resourcemanager'] | join('')}}:10020</value></property>
  <property><name>mapreduce.jobhistory.webapp.address</name><value>{{  groups['cdh5-resourcemanager'] | join('') }}:19888</value></property>

  <!-- YARN requires a staging directory for temporary files created by running jobs -->
  <property><name>yarn.app.mapreduce.am.staging-dir</name><value>/user</value></property>

  <!-- Fix the issue about memory limits -->
  <property><name>mapreduce.map.memory.mb</name><value>{{ mapreduce_map_memory_mb }}</value></property>
  <property><name>mapreduce.reduce.memory.mb</name><value>{{ mapreduce_reduce_memory_mb }}</value></property>
  <property><name>yarn.app.mapreduce.am.resource.mb</name><value>{{ yarn_app_mapreduce_am_resource_mb }}</value></property>

  <property><name>mapreduce.map.java.opts</name><value>{{ mapreduce_map_java_opts }}</value></property>
  <property><name>mapreduce.reduce.java.opts</name><value>{{ mapreduce_reduce_java_opts }}</value></property>
  <property><name>yarn.app.mapreduce.am.command-opts</name><value>{{ yarn_app_mapreduce_am_command_opts }}</value></property>

  <property><name>mapreduce.jobtracker.handler.count</name><value>{{ mapreduce_jobtracker_handler_count }}</value></property>
  <property><name>dfs.namenode.handler.count</name><value>{{ dfs_namenode_handler_count }}</value></property>

  <property><name>mapreduce.map.cpu.vcores</name><value>1</value></property>
  <property><name>mapreduce.reduce.cpu.vcores</name><value>1</value></property>
  <property><name>yarn.app.mapreduce.am.resource.cpu-vcores</name><value>1</value></property>

  <property><name>mapred.output.direct.EmrFileSystem</name><value>true</value></property>
  <property><name>mapreduce.task.io.sort.factor</name><value>48</value></property>
  <property><name>mapreduce.job.userlog.retain.hours</name><value>48</value></property>
  <property><name>mapreduce.reduce.shuffle.parallelcopies</name><value>20</value></property>

  <property><name>hadoop.job.history.user.location</name><value>none</value></property>

  <property><name>mapreduce.map.speculative</name><value>true</value></property>
  <property><name>mapreduce.reduce.speculative</name><value>true</value></property>

  <property><name>mapred.output.direct.NativeS3FileSystem</name><value>true</value></property>

  <property><name>mapreduce.job.maps</name><value>{{ mapreduce_job_maps }}</value></property>
  <property><name>mapreduce.job.reduces</name><value>{{ mapreduce_job_reduces }}</value></property>

  <property><name>mapreduce.map.output.compress</name><value>true</value></property>
  <property><name>yarn.app.mapreduce.am.job.task.listener.thread-count</name><value>60</value></property>

  <property><name>mapreduce.job.jvm.numtasks</name><value>{{ mapreduce_job_jvm_numtasks }}</value></property>

  <property><name>mapreduce.map.output.compress.codec</name><value>org.apache.hadoop.io.compress.SnappyCodec</value></property>

</configuration>
